gpu21
2 2 1, 5e-4, 1000, huber, reward_scaled, rnn
[[[tensor(0.9226, grad_fn=<AddBackward0>), tensor(0.9226, grad_fn=<AddBackward0>)], [tensor(2.8627, grad_fn=<RsubBackward1>), tensor(2.8627, grad_fn=<RsubBackward1>)]], [[tensor(0.7472, grad_fn=<AddBackward0>), tensor(0.7472, grad_fn=<AddBackward0>)], [tensor(1.5652, grad_fn=<RsubBackward1>), tensor(1.5652, grad_fn=<RsubBackward1>)]], [[tensor(0.8425, grad_fn=<AddBackward0>), tensor(0.8425, grad_fn=<AddBackward0>)], [tensor(1.3811, grad_fn=<RsubBackward1>), tensor(1.3811, grad_fn=<RsubBackward1>)]], [[tensor(1.0228, grad_fn=<AddBackward0>), tensor(1.0228, grad_fn=<AddBackward0>)], [tensor(0.5500, grad_fn=<RsubBackward1>), tensor(0.5500, grad_fn=<RsubBackward1>)]]]

gpu18
2 2 5, 5e-4, 5000, huber, reward_scaled, npa

gpu12
2 2 3, 5e-4, 5000, huber, reward_scaled, npa


gpu14
2 2 1, 5e-4, 5000, huber, reward_scaled, npa, 32
[[[tensor(1.5821, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.5821, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(1.4573, device='cuda:0', grad_fn=<RsubBackward1>), tensor(1.4573, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.5190, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.5190, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(1.3750, device='cuda:0', grad_fn=<RsubBackward1>), tensor(1.3750, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.8070, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.8070, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(1.7291, device='cuda:0', grad_fn=<RsubBackward1>), tensor(1.7291, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.4641, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.4641, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.7833, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.7833, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.5830, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.5830, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.7874, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.7874, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.5999, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.5999, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.7572, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.7572, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(2.1308, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.1308, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(1.5682, device='cuda:0', grad_fn=<RsubBackward1>), tensor(1.5682, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.7914, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.7914, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(1.6252, device='cuda:0', grad_fn=<RsubBackward1>), tensor(1.6252, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.7293, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.7293, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(1.6916, device='cuda:0', grad_fn=<RsubBackward1>), tensor(1.6916, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(2.2073, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.2073, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.4618, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.4618, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.4995, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.4995, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.3512, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.3512, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.1315, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.1315, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.3823, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.3823, device='cuda:0', grad_fn=<RsubBackward1>)]]

gpu18
2 2 1, 5e-4, 5000, mse, reward_scaled, npa
[[[tensor(1.7228, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.7228, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.4714, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.4714, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.6843, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.6843, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.0075, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.0075, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.6410, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.6410, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.1156, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.1156, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(4.3269, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.3269, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.1794, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.1794, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(4.5469, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.5469, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.0651, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.0651, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(3.8949, device='cuda:0', grad_fn=<AddBackward0>), tensor(3.8949, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.0193, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.0193, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(0.2365, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2365, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.0880, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.0880, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.1606, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.1606, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.3185, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.3185, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.4841, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.4841, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.3996, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.3996, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(3.3283, device='cuda:0', grad_fn=<AddBackward0>), tensor(3.3283, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.5426, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.5426, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(3.3184, device='cuda:0', grad_fn=<AddBackward0>), tensor(3.3184, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.2864, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.2864, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(0.4649, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.4649, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.0289, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.0289, device='cuda:0', grad_fn=<RsubBackward1>)]]]

gpu12
5 2 1, 5e-4, 5000, huber, reward_scaled, npa
[[[tensor(1.3885, grad_fn=<AddBackward0>), tensor(1.3885, grad_fn=<AddBackward0>)], [tensor(1.1645, grad_fn=<RsubBackward1>), tensor(1.1645, grad_fn=<RsubBackward1>)]], [[tensor(1.3112, grad_fn=<AddBackward0>), tensor(1.3112, grad_fn=<AddBackward0>)], [tensor(1.1074, grad_fn=<RsubBackward1>), tensor(1.1074, grad_fn=<RsubBackward1>)]], [[tensor(1.3596, grad_fn=<AddBackward0>), tensor(1.3596, grad_fn=<AddBackward0>)], [tensor(1.0473, grad_fn=<RsubBackward1>), tensor(1.0473, grad_fn=<RsubBackward1>)]], [[tensor(1.1354, grad_fn=<AddBackward0>), tensor(1.1354, grad_fn=<AddBackward0>)], [tensor(0.3020, grad_fn=<RsubBackward1>), tensor(0.3020, grad_fn=<RsubBackward1>)]], [[tensor(1.1197, grad_fn=<AddBackward0>), tensor(1.1197, grad_fn=<AddBackward0>)], [tensor(0.5638, grad_fn=<RsubBackward1>), tensor(0.5638, grad_fn=<RsubBackward1>)]], [[tensor(1.7051, grad_fn=<AddBackward0>), tensor(1.7051, grad_fn=<AddBackward0>)], [tensor(0.5115, grad_fn=<RsubBackward1>), tensor(0.5115, grad_fn=<RsubBackward1>)]], [[tensor(1.6734, grad_fn=<AddBackward0>), tensor(1.6734, grad_fn=<AddBackward0>)], [tensor(1.6704, grad_fn=<RsubBackward1>), tensor(1.6704, grad_fn=<RsubBackward1>)]], [[tensor(1.6246, grad_fn=<AddBackward0>), tensor(1.6246, grad_fn=<AddBackward0>)], [tensor(1.6782, grad_fn=<RsubBackward1>), tensor(1.6782, grad_fn=<RsubBackward1>)]], [[tensor(1.7089, grad_fn=<AddBackward0>), tensor(1.7089, grad_fn=<AddBackward0>)], [tensor(1.6674, grad_fn=<RsubBackward1>), tensor(1.6674, grad_fn=<RsubBackward1>)]], [[tensor(1.6265, grad_fn=<AddBackward0>), tensor(1.6265, grad_fn=<AddBackward0>)], [tensor(0.5025, grad_fn=<RsubBackward1>), tensor(0.5025, grad_fn=<RsubBackward1>)]], [[tensor(1.4097, grad_fn=<AddBackward0>), tensor(1.4097, grad_fn=<AddBackward0>)], [tensor(0.5591, grad_fn=<RsubBackward1>), tensor(0.5591, grad_fn=<RsubBackward1>)]], [[tensor(1.4499, grad_fn=<AddBackward0>), tensor(1.4499, grad_fn=<AddBackward0>)], [tensor(0.5504, grad_fn=<RsubBackward1>), tensor(0.5504, grad_fn=<RsubBackward1>)]]]


2 2 1, 5e-4, 1000, huber, reward_scaled, npa
[[[tensor(0.7227, grad_fn=<AddBackward0>), tensor(0.7227, grad_fn=<AddBackward0>)], [tensor(0.3579, grad_fn=<RsubBackward1>), tensor(0.3579, grad_fn=<RsubBackward1>)]], [[tensor(0.0563, grad_fn=<AddBackward0>), tensor(0.0563, grad_fn=<AddBackward0>)], [tensor(0.1114, grad_fn=<RsubBackward1>), tensor(0.1114, grad_fn=<RsubBackward1>)]], [[tensor(0.7173, grad_fn=<AddBackward0>), tensor(0.7173, grad_fn=<AddBackward0>)], [tensor(0.3897, grad_fn=<RsubBackward1>), tensor(0.3897, grad_fn=<RsubBackward1>)]], [[tensor(3.3873, grad_fn=<AddBackward0>), tensor(3.3873, grad_fn=<AddBackward0>)], [tensor(0.3429, grad_fn=<RsubBackward1>), tensor(0.3429, grad_fn=<RsubBackward1>)]], [[tensor(3.3407, grad_fn=<AddBackward0>), tensor(3.3407, grad_fn=<AddBackward0>)], [tensor(0.1559, grad_fn=<RsubBackward1>), tensor(0.1559, grad_fn=<RsubBackward1>)]], [[tensor(3.4079, grad_fn=<AddBackward0>), tensor(3.4079, grad_fn=<AddBackward0>)], [tensor(0.0318, grad_fn=<RsubBackward1>), tensor(0.0318, grad_fn=<RsubBackward1>)]], [[tensor(0.2512, grad_fn=<AddBackward0>), tensor(0.2512, grad_fn=<AddBackward0>)], [tensor(0.6498, grad_fn=<RsubBackward1>), tensor(0.6498, grad_fn=<RsubBackward1>)]], [[tensor(0.6212, grad_fn=<AddBackward0>), tensor(0.6212, grad_fn=<AddBackward0>)], [tensor(0.6339, grad_fn=<RsubBackward1>), tensor(0.6339, grad_fn=<RsubBackward1>)]], [[tensor(0.5669, grad_fn=<AddBackward0>), tensor(0.5669, grad_fn=<AddBackward0>)], [tensor(0.6457, grad_fn=<RsubBackward1>), tensor(0.6457, grad_fn=<RsubBackward1>)]], [[tensor(2.5864, grad_fn=<AddBackward0>), tensor(2.5864, grad_fn=<AddBackward0>)], [tensor(0.5142, grad_fn=<RsubBackward1>), tensor(0.5142, grad_fn=<RsubBackward1>)]], [[tensor(2.4295, grad_fn=<AddBackward0>), tensor(2.4295, grad_fn=<AddBackward0>)], [tensor(0.1277, grad_fn=<RsubBackward1>), tensor(0.1277, grad_fn=<RsubBackward1>)]], [[tensor(2.4375, grad_fn=<AddBackward0>), tensor(2.4375, grad_fn=<AddBackward0>)], [tensor(0.2677, grad_fn=<RsubBackward1>), tensor(0.2677, grad_fn=<RsubBackward1>)]]] 

gpu14
2 2 1, 5e-4, 1000, huber, npa
[[[tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.3901, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.3901, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.3351, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.3351, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.1321, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.1321, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(0.9022, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.9022, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.0734, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.0734, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(4.2816, device='cuda:0', grad_fn=<AddBackward0>), tensor(4.2816, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.2170, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.2170, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(3.9105, device='cuda:0', grad_fn=<AddBackward0>), tensor(3.9105, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.0894, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.0894, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(3.3432, device='cuda:0', grad_fn=<AddBackward0>), tensor(3.3432, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.0448, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.0448, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.7548, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.7548, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(0.5997, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.5997, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.6898, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.6898, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(0.6137, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.6137, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.7152, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.7152, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(2.6558, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.6558, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.3919, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.3919, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(2.4281, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.4281, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.2703, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.2703, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(2.0744, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.0744, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.2790, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.2790, device='cuda:0', grad_fn=<RsubBackward1>)]]]

rnn
betas = [[0.9, 0.999], [0.9, 0.999]]
500000
lr=1e-3
gpu21/sec
PBNE: [tensor(22.8877, grad_fn=<SubBackward0>), tensor(17.4877, grad_fn=<SubBackward0>)] tensor(0.0364, grad_fn=<RsubBackward1>)                                                                                                                BR: [tensor(13.6084, grad_fn=<AddBackward0>), tensor(11.5854, grad_fn=<AddBackward0>)] 7.3655023878253                  Overall: [tensor(22.8877, grad_fn=<AddBackward0>), tensor(17.4877, grad_fn=<AddBackward0>)] tensor(0.0364, grad_fn=<RsubBackward1>) 

iterated:
PBNE: [tensor(0.2960, grad_fn=<SubBackward0>), tensor(0.2309, grad_fn=<SubBackward0>)] tensor(32.4052, grad_fn=<RsubBackward1>)                                                                                                                 BR: [tensor(13.3119, grad_fn=<AddBackward0>), tensor(11.4255, grad_fn=<AddBackward0>)] 20.29237655110624                Overall: [tensor(0.2960, grad_fn=<AddBackward0>), tensor(0.2309, grad_fn=<AddBackward0>)] tensor(32.4052, grad_fn=<RsubBackward1>) 

betas = [[0, 0.99], [0, 0.99]]:

rnn
21
huber

PBNE: [tensor(0.8680, grad_fn=<SubBackward0>), tensor(0.7692, grad_fn=<SubBackward0>)] tensor(2.0571, grad_fn=<RsubBackward1>)                                                                                                                  BR: [tensor(2.4063, grad_fn=<AddBackward0>), tensor(0.5032, grad_fn=<AddBackward0>)] 1.4209928650788013                 Overall: [tensor(0.8680, grad_fn=<AddBackward0>), tensor(0.7692, grad_fn=<AddBackward0>)] tensor(2.0571, grad_fn=<RsubBackward1>)          

npa

12
huber

[[[tensor(0.1206, grad_fn=<AddBackward0>), tensor(0.1206, grad_fn=<AddBackward0>)], [tensor(1.2189, grad_fn=<RsubBackward1>), tensor(1.2189, grad_fn=<RsubBackward1>)]], [[tensor(0.0084, grad_fn=<AddBackward0>), tensor(0.0084, grad_fn=<AddBackward0>)], [tensor(0.6632, grad_fn=<RsubBackward1>), tensor(0.6632, grad_fn=<RsubBackward1>)]], [[tensor(4.3863, grad_fn=<AddBackward0>), tensor(4.3863, grad_fn=<AddBackward0>)], [tensor(1.7874, grad_fn=<RsubBackward1>), tensor(1.7874, grad_fn=<RsubBackward1>)]]]         

18
mse

rms
1e-3
1000 batch size
gpu12
-

betas = [[0, 0.99], [0, 0.99]]
lr:5e-5
100000
gpu12/npa
-


npa
    betas = [[0, 0.99], [0.9, 0.99]]
lr:5e-5
100000
gpu14/npa
PBNE: [tensor(2.6252, device='cuda:0', grad_fn=<SubBackward0>), tensor(2.6252, device='cuda:0', grad_fn=<SubBackward0>)] tensor(2.2245, device='cuda:0', grad_fn=<RsubBackward1>)                                                               BR: [tensor(-0.8995, device='cuda:0', grad_fn=<AddBackward0>), tensor(-0.8995, device='cuda:0', grad_fn=<AddBackward0>)] 8.054699740285926                                                                                                      Overall: [tensor(2.6252, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.6252, device='cuda:0', grad_fn=<AddBackward0>)] tensor(2.2245, device='cuda:0', grad_fn=<RsubBackward1>)    

betas = [[0, 0.99], [0, 0.99]]
1e-4
100000
gpu18/sec
PBNE: [tensor(1.2668, device='cuda:0', grad_fn=<SubBackward0>), tensor(1.2668, device='cuda:0', grad_fn=<SubBackward0>)] tensor(4.5951, device='cuda:0', grad_fn=<RsubBackward1>)                                                               BR: [tensor(-2.4859, device='cuda:0', grad_fn=<AddBackward0>), tensor(-2.4859, device='cuda:0', grad_fn=<AddBackward0>)] 8.789309381236205                                                                                                      Overall: [tensor(1.2668, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.2668, device='cuda:0', grad_fn=<AddBackward0>)] tensor(4.5951, device='cuda:0', grad_fn=<RsubBackward1>)   

    betas = [[0, 0.999], [0, 0.99]]
    5e-5
    20000
    gpu14/npa_2

betas = [[0, 0.999], [0, 0.99]]
    5e-4
    20000
    gpu12/npa
    [[[tensor(3.8805, grad_fn=<AddBackward0>), tensor(3.8805, grad_fn=<AddBackward0>)], [tensor(2.6752, grad_fn=<RsubBackward1>), tensor(2.6752, grad_fn=<RsubBackward1>)]], [[tensor(3.3275, grad_fn=<AddBackward0>), tensor(3.3275, grad_fn=<AddBackward0>)], [tensor(2.7036, grad_fn=<RsubBackward1>), tensor(2.7036, grad_fn=<RsubBackward1>)]], [[tensor(2.9415, grad_fn=<AddBackward0>), tensor(2.9415, grad_fn=<AddBackward0>)], [tensor(1.3128, grad_fn=<RsubBackward1>), tensor(1.3128, grad_fn=<RsubBackward1>)]]]  

rms 
weight_decay=0
1e-5
gpu12


PBNE: [tensor(3.5874, grad_fn=<SubBackward0>), tensor(3.5874, grad_fn=<SubBackward0>)] tensor(1.8740, grad_fn=<RsubBackward1>)                                                                                                                  BR: [tensor(-0.3546, grad_fn=<AddBackward0>), tensor(-0.3546, grad_fn=<AddBackward0>)] 7.885191666535906                Overall: [tensor(3.5874, grad_fn=<AddBackward0>), tensor(3.5874, grad_fn=<AddBackward0>)] tensor(1.8740, grad_fn=<RsubBackward1>)                                                                                                               [[[tensor(3.5832, grad_fn=<AddBackward0>), tensor(3.5832, grad_fn=<AddBackward0>)], [tensor(1.7694, grad_fn=<RsubBackward1>), tensor(1.7694, grad_fn=<RsubBackward1>)]], [[tensor(3.5708, grad_fn=<AddBackward0>), tensor(3.5708, grad_fn=<AddBackward0>)], [tensor(1.8384, grad_fn=<RsubBackward1>), tensor(1.8384, grad_fn=<RsubBackward1>)]], [[tensor(3.5874, grad_fn=<AddBackward0>), tensor(3.5874, grad_fn=<AddBackward0>)], [tensor(1.8740, grad_fn=<RsubBackward1>), tensor(1.8740, grad_fn=<RsubBackward1>)]]]  

rms
weight_decay=0.99
1e-4
gpu18

PBNE: [tensor(3.1156, device='cuda:0', grad_fn=<SubBackward0>), tensor(3.1156, device='cuda:0', grad_fn=<SubBackward0>)] tensor(1.1279, device='cuda:0', grad_fn=<RsubBackward1>)                                                               BR: [tensor(-0.5443, device='cuda:0', grad_fn=<AddBackward0>), tensor(-0.5443, device='cuda:0', grad_fn=<AddBackward0>)] 6.918767453018739                                                                                                      Overall: [tensor(3.1156, device='cuda:0', grad_fn=<AddBackward0>), tensor(3.1156, device='cuda:0', grad_fn=<AddBackward0>)] tensor(1.1279, device='cuda:0', grad_fn=<RsubBackward1>)    

rms
weight_decay=0.99
5e-4
gpu14

PBNE: [tensor(3.0272, device='cuda:0', grad_fn=<SubBackward0>), tensor(3.0272, device='cuda:0', grad_fn=<SubBackward0>)] tensor(0.9200, device='cuda:0', grad_fn=<RsubBackward1>)                                                               BR: [tensor(-0.8301, device='cuda:0', grad_fn=<AddBackward0>), tensor(-0.8301, device='cuda:0', grad_fn=<AddBackward0>)] 6.8447136741307295                                                                                                     Overall: [tensor(3.0272, device='cuda:0', grad_fn=<AddBackward0>), tensor(3.0272, device='cuda:0', grad_fn=<AddBackward0>)] tensor(0.9200, device='cuda:0', grad_fn=<RsubBackward1>)                                                            [[[tensor(2.8590, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.8590, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(1.2052, device='cuda:0', grad_fn=<RsubBackward1>), tensor(1.2052, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(2.6394, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.6394, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(1.0990, device='cuda:0', grad_fn=<RsubBackward1>), tensor(1.0990, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(3.0272, device='cuda:0', grad_fn=<AddBackward0>), tensor(3.0272, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(0.9200, device='cuda:0', grad_fn=<RsubBackward1>), tensor(0.9200, device='cuda:0', grad_fn=<RsubBackward1>)]]] 

rms
weight_decay=0.99
5e-5
gpu12

[[[tensor(2.6347, grad_fn=<AddBackward0>), tensor(2.6347, grad_fn=<AddBackward0>)], [tensor(1.1406, grad_fn=<RsubBackward1>), tensor(1.1406, grad_fn=<RsubBackward1>)]], [[tensor(2.8197, grad_fn=<AddBackward0>), tensor(2.8197, grad_fn=<AddBackward0>)], [tensor(1.4090, grad_fn=<RsubBackward1>), tensor(1.4090, grad_fn=<RsubBackward1>)]], [[tensor(3.1597, grad_fn=<AddBackward0>), tensor(3.1597, grad_fn=<AddBackward0>)], [tensor(0.9919, grad_fn=<RsubBackward1>), tensor(0.9919, grad_fn=<RsubBackward1>)]]]  

rms
weight_decay=0.99
1e-3
gpu14

>)] tensor(1.1510, device='cuda:0', grad_fn=<RsubBackward1>)                                                            [[[tensor(1.4268, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.4268, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(2.0995, device='cuda:0', grad_fn=<RsubBackward1>), tensor(2.0995, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.5920, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.5920, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(1.2475, device='cuda:0', grad_fn=<RsubBackward1>), tensor(1.2475, device='cuda:0', grad_fn=<RsubBackward1>)]], [[tensor(1.3193, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.3193, device='cuda:0', grad_fn=<AddBackward0>)], [tensor(1.1510, device='cuda:0', grad_fn=<RsubBackward1>), tensor(1.1510, device='cuda:0', grad_fn=<RsubBackward1>)]]]  

rms
weight_decay=0.99
1e-3
gpu18

PBNE: [tensor(2.1319, device='cuda:0', grad_fn=<SubBackward0>), tensor(2.1319, device='cuda:0', grad_fn=<SubBackward0>)] tensor(0.7484, device='cuda:0', grad_fn=<RsubBackward1>)                                                               BR: [tensor(-1.6559, device='cuda:0', grad_fn=<AddBackward0>), tensor(-1.6559, device='cuda:0', grad_fn=<AddBackward0>)] 6.577728255209422                                                                                                      Overall: [tensor(2.1319, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.1319, device='cuda:0', grad_fn=<AddBackward0>)] tensor(0.7484, device='cuda:0', grad_fn=<RsubBackward1>)   

rms
weight_decay=0.99
1e-3
1000 batch size
new env
gpu18
  

rnn+ 5 10: 21/sec



npa 5 10:21/sec_2


npa 10 10:14/npa


npa 10 5: 14/npa
PBNE: [tensor(15.2592, device='cuda:0', grad_fn=<SubBackward0>), tensor(11.4564, device='cuda:0', grad_fn=<SubBackward0>)] tensor(31.9413, device='cuda:0', grad_fn=<RsubBackward1>)                                                            BR: [tensor(41.9701, device='cuda:0', grad_fn=<AddBackward0>), tensor(38.1674, device='cuda:0', grad_fn=<AddBackward0>)] 3.736849289627095                                                                                                      Overall: [tensor(15.2592, device='cuda:0', grad_fn=<AddBackward0>), tensor(11.4564, device='cuda:0', grad_fn=<AddBackward0>)] tensor(31.9413, device='cuda:0', grad_fn=<RsubBackward1>)

rnn+ 10 5: 12/npa 

PBNE: [tensor(11.9191, grad_fn=<SubBackward0>), tensor(9.0475, grad_fn=<SubBackward0>)] tensor(2.1236, grad_fn=<RsubBackward1>)                                                                                                                 BR: [tensor(40.6937, grad_fn=<AddBackward0>), tensor(38.9065, grad_fn=<AddBackward0>)] -27.198566030735133              Overall: [tensor(11.9191, grad_fn=<AddBackward0>), tensor(9.0475, grad_fn=<AddBackward0>)] tensor(2.1236, grad_fn=<RsubBackward1>)   

1                                                                                                               [[-5.7555876  7.686729   7.686729   7.686729   7.686729   7.686729                                                 7.686729   7.686729   7.686729   7.686729 ]                                                                   [ 8.168497  -9.78248    8.168497   8.168497   8.168497   8.168497                                                 8.168497   8.168497   8.168497   8.168497 ]                                                                   [ 5.1357493  5.1357493 -8.279294   5.1357493  5.1357493  5.1357493                                                5.1357493  5.1357493  5.1357493  5.1357493]                                                                   [ 5.475095   5.475095   5.475095  -5.4395266  5.475095   5.475095                                                 5.475095   5.475095   5.475095   5.475095 ]                                                                   [ 9.642369   9.642369   9.642369   9.642369  -9.089085   9.642369                                                 9.642369   9.642369   9.642369   9.642369 ]                                                                   [ 6.2038617  6.2038617  6.2038617  6.2038617  6.2038617 -7.7264996                                                6.2038617  6.2038617  6.2038617  6.2038617]                                                                   [ 7.355336   7.355336   7.355336   7.355336   7.355336   7.355336                                                -9.700486   7.355336   7.355336   7.355336 ]                                                                   [ 9.408173   9.408173   9.408173   9.408173   9.408173   9.408173                                                 9.408173  -7.1601     9.408173   9.408173 ]                                                                   [ 7.597496   7.597496   7.597496   7.597496   7.597496   7.597496                                                 7.597496   7.597496  -8.18966    7.597496 ]                                                                   [ 7.787985   7.787985   7.787985   7.787985   7.787985   7.787985                                                 7.787985   7.787985   7.787985  -6.6674075]]                                                                 [[ 5.7555876 -7.686729  -7.686729  -7.686729  -7.686729  -7.686729                                                -7.686729  -7.686729  -7.686729  -7.686729 ]                                                                   [-8.168497   9.78248   -8.168497  -8.168497  -8.168497  -8.168497                                                -8.168497  -8.168497  -8.168497  -8.168497 ]                                                                   [-5.1357493 -5.1357493  8.279294  -5.1357493 -5.1357493 -5.1357493                                               -5.1357493 -5.1357493 -5.1357493 -5.1357493]                                                                   [-5.475095  -5.475095  -5.475095   5.4395266 -5.475095  -5.475095                                                -5.475095  -5.475095  -5.475095  -5.475095 ]                                                                   [-9.642369  -9.642369  -9.642369  -9.642369   9.089085  -9.642369                                                -9.642369  -9.642369  -9.642369  -9.642369 ]                                                                   [-6.2038617 -6.2038617 -6.2038617 -6.2038617 -6.2038617  7.7264996                                               -6.2038617 -6.2038617 -6.2038617 -6.2038617]                                                                   [-7.355336  -7.355336  -7.355336  -7.355336  -7.355336  -7.355336                                                 9.700486  -7.355336  -7.355336  -7.355336 ]                                                                   [-9.408173  -9.408173  -9.408173  -9.408173  -9.408173  -9.408173                                                -9.408173   7.1601    -9.408173  -9.408173 ]                                                                   [-7.597496  -7.597496  -7.597496  -7.597496  -7.597496  -7.597496                                                -7.597496  -7.597496   8.18966   -7.597496 ]                                                                   [-7.787985  -7.787985  -7.787985  -7.787985  -7.787985  -7.787985                                                -7.787985  -7.787985  -7.787985   6.6674075]]                                                                 [-3690.4092   -125.02117  -338.84808  -310.3595   -177.76207  -187.63585                                          -199.26741  -226.57607  -283.87964  -257.9388 ]  

rnn 5type 10 5:
   5.491767   5.491767   5.491767   5.491767 ]                                                                      [ 7.709198   7.709198   7.709198   7.709198   7.709198   7.709198                                                   -6.4887743  7.709198   7.709198   7.709198 ]                                                                      [ 9.0245285  9.0245285  9.0245285  9.0245285  9.0245285  9.0245285                                                   9.0245285 -9.023113   9.0245285  9.0245285]                                                                      [ 9.281104   9.281104   9.281104   9.281104   9.281104   9.281104                                                    9.281104   9.281104  -5.9906116  9.281104 ]                                                                      [ 7.0045767  7.0045767  7.0045767  7.0045767  7.0045767  7.0045767                                                   7.0045767  7.0045767  7.0045767 -9.697398 ]]                                                                    [[ 9.167827  -6.377258  -6.377258  -6.377258  -6.377258  -6.377258                                                   -6.377258  -6.377258  -6.377258  -6.377258 ]                                                                      [-6.6682277  9.225978  -6.6682277 -6.6682277 -6.6682277 -6.6682277                                                  -6.6682277 -6.6682277 -6.6682277 -6.6682277]                                                                      [-5.4421277 -5.4421277  7.6736116 -5.4421277 -5.4421277 -5.4421277                                                  -5.4421277 -5.4421277 -5.4421277 -5.4421277]                                                                      [-7.8916855 -7.8916855 -7.8916855  5.731701  -7.8916855 -7.8916855                                                  -7.8916855 -7.8916855 -7.8916855 -7.8916855]                                                                      [-7.1361427 -7.1361427 -7.1361427 -7.1361427  5.4578176 -7.1361427                                                  -7.1361427 -7.1361427 -7.1361427 -7.1361427]                                                                      [-5.491767  -5.491767  -5.491767  -5.491767  -5.491767   6.332867                                                   -5.491767  -5.491767  -5.491767  -5.491767 ]                                                                      [-7.709198  -7.709198  -7.709198  -7.709198  -7.709198  -7.709198                                                    6.4887743 -7.709198  -7.709198  -7.709198 ]                                                                      [-9.0245285 -9.0245285 -9.0245285 -9.0245285 -9.0245285 -9.0245285                                                  -9.0245285  9.023113  -9.0245285 -9.0245285]                                                                      [-9.281104  -9.281104  -9.281104  -9.281104  -9.281104  -9.281104                                                   -9.281104  -9.281104   5.9906116 -9.281104 ]                                                                      [-7.0045767 -7.0045767 -7.0045767 -7.0045767 -7.0045767 -7.0045767                                                  -7.0045767 -7.0045767 -7.0045767  9.697398 ]]                                                                    [-3438.5308   -310.6791   -271.67615  -248.42574  -235.27144  -142.4668                                              -214.51147  -193.01833  -163.5165   -173.86775]  

   PBNE: [tensor(11.9020, grad_fn=<SubBackward0>), tensor(10.0705, grad_fn=<SubBackward0>), tensor(12.5349, grad_fn=<SubBackward0>), tensor(10.1451, grad_fn=<SubBackward0>), tensor(9.5984, grad_fn=<SubBackward0>)] tensor(0.8960, grad_fn=<RsubBackward1>)                                                                                               BR: [tensor(40.4978, grad_fn=<AddBackward0>), tensor(40.1557, grad_fn=<AddBackward0>), tensor(41.9044, grad_fn=<AddBackward0>), tensor(41.6062, grad_fn=<AddBackward0>), tensor(38.4589, grad_fn=<AddBackward0>)] -28.765206672041504 Overall: [tensor(11.9020, grad_fn=<AddBackward0>), tensor(10.0705, grad_fn=<AddBackward0>), tensor(12.5349, grad_fn=<AddBackward0>), tensor(10.1451, grad_fn=<AddBackward0>), tensor(9.5984, grad_fn=<AddBackward0>)] tensor(0.8960, grad_fn=<RsubBackward1>) 

   rnn 10type 10 5 gpu21/sec:

 [ 6.332867   6.332867   6.332867   6.332867   6.332867  -6.2737966                                                        6.332867   6.332867   6.332867   6.332867 ]                                                                           [ 6.4887743  6.4887743  6.4887743  6.4887743  6.4887743  6.4887743                                                       -8.195396   6.4887743  6.4887743  6.4887743]                                                                           [ 9.023113   9.023113   9.023113   9.023113   9.023113   9.023113                                                         9.023113  -7.577223   9.023113   9.023113 ]                                                                           [ 5.9906116  5.9906116  5.9906116  5.9906116  5.9906116  5.9906116                                                        5.9906116  5.9906116 -7.086194   5.9906116]                                                                           [ 9.697398   9.697398   9.697398   9.697398   9.697398   9.697398                                                         9.697398   9.697398   9.697398  -6.9504027]]                                                                         [[ 8.254523  -9.167827  -9.167827  -9.167827  -9.167827  -9.167827                                                        -9.167827  -9.167827  -9.167827  -9.167827 ]                                                                           [-9.225978   9.105898  -9.225978  -9.225978  -9.225978  -9.225978                                                        -9.225978  -9.225978  -9.225978  -9.225978 ]                                                                           [-7.6736116 -7.6736116  9.229596  -7.6736116 -7.6736116 -7.6736116                                                       -7.6736116 -7.6736116 -7.6736116 -7.6736116]                                                                           [-5.731701  -5.731701  -5.731701   6.93905   -5.731701  -5.731701                                                        -5.731701  -5.731701  -5.731701  -5.731701 ]                                                                           [-5.4578176 -5.4578176 -5.4578176 -5.4578176  7.5304008 -5.4578176                                                       -5.4578176 -5.4578176 -5.4578176 -5.4578176]                                                                           [-6.332867  -6.332867  -6.332867  -6.332867  -6.332867   6.2737966                                                       -6.332867  -6.332867  -6.332867  -6.332867 ]                                                                           [-6.4887743 -6.4887743 -6.4887743 -6.4887743 -6.4887743 -6.4887743                                                        8.195396  -6.4887743 -6.4887743 -6.4887743]                                                                           [-9.023113  -9.023113  -9.023113  -9.023113  -9.023113  -9.023113                                                        -9.023113   7.577223  -9.023113  -9.023113 ]                                                                           [-5.9906116 -5.9906116 -5.9906116 -5.9906116 -5.9906116 -5.9906116                                                       -5.9906116 -5.9906116  7.086194  -5.9906116]                                                                           [-9.697398  -9.697398  -9.697398  -9.697398  -9.697398  -9.697398                                                        -9.697398  -9.697398  -9.697398   6.9504027]]                                                                         [-3774.7097   -327.20358  -296.94415  -282.15878  -276.17917  -223.02011                                                  -188.03508  -169.63892  -159.57109  -164.30447]                                                                       train finish                                                                                                            atk br calculated                                                                                                       br calculate finish                                                                                                     atk utility calculated                                                                                                  PBNE: [tensor(12.1100, grad_fn=<SubBackward0>), tensor(9.8808, grad_fn=<SubBackward0>), tensor(11.9578, grad_fn=<SubBackward0>), tensor(8.6155, grad_fn=<SubBackward0>), tensor(8.2012, grad_fn=<SubBackward0>), tensor(9.6719, grad_fn=<SubBackward0>), tensor(10.2835, grad_fn=<SubBackward0>), tensor(5.8639, grad_fn=<SubBackward0>), tensor(8.3038, grad_fn=<SubBackward0>), tensor(10.5656, grad_fn=<SubBackward0>)] tensor(0.7193, grad_fn=<RsubBackward1>)                              BR: [tensor(40.7570, grad_fn=<AddBackward0>), tensor(39.5156, grad_fn=<AddBackward0>), tensor(40.3781, grad_fn=<AddBackward0>), tensor(40.2715, grad_fn=<AddBackward0>), tensor(37.1418, grad_fn=<AddBackward0>), tensor(39.5291, grad_fn=<AddBackward0>), tensor(38.0056, grad_fn=<AddBackward0>), tensor(29.7256, grad_fn=<AddBackward0>), tensor(36.7379, grad_fn=<AddBackward0>), tensor(40.3948, grad_fn=<AddBackward0>)] -27.965042240699066                                              Overall: [tensor(12.1100, grad_fn=<AddBackward0>), tensor(9.8808, grad_fn=<AddBackward0>), tensor(11.9578, grad_fn=<AddBackward0>), tensor(8.6155, grad_fn=<AddBackward0>), tensor(8.2012, grad_fn=<AddBackward0>), tensor(9.6719, grad_fn=<AddBackward0>), tensor(10.2835, grad_fn=<AddBackward0>), tensor(5.8639, grad_fn=<AddBackward0>), tensor(8.3038, grad_fn=<AddBackward0>), tensor(10.5656, grad_fn=<AddBackward0>)] tensor(0.7193, grad_fn=<RsubBackward1>)  

 rnn 10type 10 10 12/npa:

  [ 9.023113   9.023113   9.023113   9.023113   9.023113   9.023113                                                         9.023113  -7.577223   9.023113   9.023113 ]                                                                           [ 5.9906116  5.9906116  5.9906116  5.9906116  5.9906116  5.9906116                                                        5.9906116  5.9906116 -7.086194   5.9906116]                                                                           [ 9.697398   9.697398   9.697398   9.697398   9.697398   9.697398                                                         9.697398   9.697398   9.697398  -6.9504027]]                                                                         [[ 8.254523  -9.167827  -9.167827  -9.167827  -9.167827  -9.167827                                                        -9.167827  -9.167827  -9.167827  -9.167827 ]                                                                           [-9.225978   9.105898  -9.225978  -9.225978  -9.225978  -9.225978                                                        -9.225978  -9.225978  -9.225978  -9.225978 ]                                                                           [-7.6736116 -7.6736116  9.229596  -7.6736116 -7.6736116 -7.6736116                                                       -7.6736116 -7.6736116 -7.6736116 -7.6736116]                                                                           [-5.731701  -5.731701  -5.731701   6.93905   -5.731701  -5.731701                                                        -5.731701  -5.731701  -5.731701  -5.731701 ]                                                                           [-5.4578176 -5.4578176 -5.4578176 -5.4578176  7.5304008 -5.4578176                                                       -5.4578176 -5.4578176 -5.4578176 -5.4578176]                                                                           [-6.332867  -6.332867  -6.332867  -6.332867  -6.332867   6.2737966                                                       -6.332867  -6.332867  -6.332867  -6.332867 ]                                                                           [-6.4887743 -6.4887743 -6.4887743 -6.4887743 -6.4887743 -6.4887743                                                        8.195396  -6.4887743 -6.4887743 -6.4887743]                                                                           [-9.023113  -9.023113  -9.023113  -9.023113  -9.023113  -9.023113                                                        -9.023113   7.577223  -9.023113  -9.023113 ]                                                                           [-5.9906116 -5.9906116 -5.9906116 -5.9906116 -5.9906116 -5.9906116                                                       -5.9906116 -5.9906116  7.086194  -5.9906116]                                                                           [-9.697398  -9.697398  -9.697398  -9.697398  -9.697398  -9.697398                                                        -9.697398  -9.697398  -9.697398   6.9504027]]                                                                         [-3774.7097   -327.20358  -296.94415  -282.15878  -276.17917  -223.02011                                                  -188.03508  -169.63892  -159.57109  -164.30447]                                                                       train finish                                                                                                            atk br calculated                                                                                                       br calculate finish                                                                                                     atk utility calculated                                                                                                  PBNE: [tensor(12.1804, grad_fn=<SubBackward0>), tensor(9.9366, grad_fn=<SubBackward0>), tensor(11.9465, grad_fn=<SubBackward0>), tensor(8.6042, grad_fn=<SubBackward0>), tensor(8.1834, grad_fn=<SubBackward0>), tensor(9.7375, grad_fn=<SubBackward0>), tensor(10.2491, grad_fn=<SubBackward0>), tensor(5.9273, grad_fn=<SubBackward0>), tensor(8.2391, grad_fn=<SubBackward0>), tensor(10.5313, grad_fn=<SubBackward0>)] tensor(0.7038, grad_fn=<RsubBackward1>)                              BR: [tensor(40.8198, grad_fn=<AddBackward0>), tensor(39.5784, grad_fn=<AddBackward0>), tensor(40.3945, grad_fn=<AddBackward0>), tensor(40.2873, grad_fn=<AddBackward0>), tensor(37.1379, grad_fn=<AddBackward0>), tensor(39.5850, grad_fn=<AddBackward0>), tensor(37.9577, grad_fn=<AddBackward0>), tensor(29.7885, grad_fn=<AddBackward0>), tensor(36.6904, grad_fn=<AddBackward0>), tensor(40.3425, grad_fn=<AddBackward0>)] -27.985513160923077                                              Overall: [tensor(12.1804, grad_fn=<AddBackward0>), tensor(9.9366, grad_fn=<AddBackward0>), tensor(11.9465, grad_fn=<AddBackward0>), tensor(8.6042, grad_fn=<AddBackward0>), tensor(8.1834, grad_fn=<AddBackward0>), tensor(9.7375, grad_fn=<AddBackward0>), tensor(10.2491, grad_fn=<AddBackward0>), tensor(5.9273, grad_fn=<AddBackward0>), tensor(8.2391, grad_fn=<AddBackward0>), tensor(10.5313, grad_fn=<AddBackward0>)] tensor(0.7038, grad_fn=<RsubBackward1>)   


npa 5type 10 5: 12/npa